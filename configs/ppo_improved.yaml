# Improved PPO configuration for better training
env:
  # Market parameters
  initial_price: 100.0
  volatility: 0.02
  drift: 0.0
  spread_mean: 0.01
  spread_std: 0.005
  
  # Fill model parameters
  fill_beta: 2.0
  fill_alpha: 0.1
  
  # Reward parameters
  inventory_penalty: 0.01
  transaction_cost: 0.001
  
  # Episode parameters
  max_steps: 1000
  episode_length: 1000
  
  # Realism parameters
  adverse_selection: true
  volatility_aware_fills: true
  enhanced_inventory_costs: true
  latency_steps: 2
  slippage_factor: 0.1

# PPO hyperparameters
learning_rate: 0.0001  # Lower learning rate for stability
n_steps: 4096          # Larger rollout buffer
batch_size: 128        # Larger batch size
n_epochs: 8            # More epochs per update
gamma: 0.995           # Higher discount factor
gae_lambda: 0.98       # Higher GAE lambda
clip_range: 0.15       # Smaller clip range for stability
vf_coef: 0.5          # Value function coefficient
ent_coef: 0.02        # Higher entropy coefficient for exploration
max_grad_norm: 0.5    # Gradient clipping

# Training parameters
total_timesteps: 500000  # Longer training
save_freq: 25000         # Save checkpoints more frequently
log_dir: "logs"
