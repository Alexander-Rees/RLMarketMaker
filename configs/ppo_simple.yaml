# Simple PPO training configuration
# Designed to work without complex wrappers

# Algorithm parameters
learning_rate: 0.0003
n_steps: 1024
batch_size: 64
n_epochs: 10
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5

# Training parameters
total_timesteps: 50000
log_interval: 10

# Environment parameters
env_id: "MarketMaker-v0"
normalize_observations: true
normalize_rewards: false

# Market dynamics - Challenging but not extreme
mu: 0.0
sigma: 0.3
initial_price: 100.0
spread_mean: 0.02
spread_vol: 0.01
lambda_orders: 5.0
episode_length: 1000
dt: 0.001

# Domain randomization
volatility_range: [0.25, 0.35]
spread_range: [0.015, 0.025]

# Fill model
fill_alpha: 0.7
fill_beta: 3.0

# Risk controls
max_inventory: 500.0
max_loss_per_episode: 500.0
kill_switch_threshold: -1000.0

# Reward parameters
lambda_inventory: 0.02
transaction_fee: 0.0002

# Logging
log_dir: "logs"
tensorboard_log: "logs/tensorboard"
verbose: 1
