# PPO training configuration with tuned environment
# Designed to train against more challenging market conditions

# Algorithm parameters
learning_rate: 0.0003
n_steps: 2048
batch_size: 64
n_epochs: 10
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5

# Training parameters
total_timesteps: 1000000
eval_freq: 10000
save_freq: 50000
log_interval: 10

# Environment parameters
env_id: "MarketMaker-v0"
normalize_observations: true
normalize_rewards: false

# Tuned market dynamics - Much more challenging
mu: 0.0                    # No drift
sigma: 0.5                 # Much higher volatility
initial_price: 100.0
spread_mean: 0.05          # Much wider spreads
spread_vol: 0.02           # High spread volatility
lambda_orders: 2.0         # Much lower order frequency
episode_length: 1000
dt: 0.001

# Domain randomization - Very challenging
volatility_range: [0.4, 0.6]
spread_range: [0.03, 0.07]

# Fill model - Much more realistic
fill_alpha: 0.5            # Much lower base fill probability
fill_beta: 5.0             # Much faster decay

# Risk controls - Stricter
max_inventory: 500.0
max_loss_per_episode: 500.0
kill_switch_threshold: -2000.0

# Reward parameters - More penalizing
lambda_inventory: 0.02    # Higher inventory penalty
transaction_fee: 0.0002   # Higher transaction fees

# Curriculum learning
curriculum: true
initial_fee_multiplier: 0.1
final_fee_multiplier: 1.0
curriculum_steps: 500000

# Logging
log_dir: "logs"
tensorboard_log: "logs/tensorboard"
verbose: 1
